Has Area Under the ROC Curve (AUC-ROC) become Data Science & AI/ML community’s P-Value?

Just returned from day 1 of Intelligent Health AI conference - and while there were some great speakers & talks - one thing stood out. Of the multiple talks reporting machine learning model performance, all except one talk reported AUC-ROC as the only metric - even for unbalanced datasets. It appears that the AUC-ROC metric is being misused similar to how the P-value has been misused & misinterpreted.

There is more to model evaluation than a single number. In addition to AUC-ROC, we have the Precision-Recall (PR) curve, Sensitivity (Recall), Specificity, F1-score, Positive/Negative Predictive Values, Matthews Correlation Coefficient, Calibration, and many other metrics. The graphic below presents a good summary of the various model performance / evaluation metrics (see articles & book for more details):

Regression Metrics: https://lnkd.in/eRWvRVc

Classification Metrics: https://lnkd.in/dpYnvGh

Evaluating Machine Learning Models (open-access book): https://lnkd.in/dHcfZdP

https://media.licdn.com/dms/document/C4E1FAQFHpKnjgdNQrg/feedshare-document-pdf-analyzed/0?e=1568451600&v=beta&t=q6_VBqTUA1mUKyuFpu-KfidLyFtU3PdLOkKCApnIUjs

Indeed, often you see miss judgment of AUC ROC. A high level of AUC could be worthless in many scenarios. 
For example, a model with a constant prediction that nobody dies due to Ebola will have AUC > 0.9999 but is wrong. 
Here are three of the essential things to evaluate: Model performance in different deciles of risk should be assessed 
(e.g., Hosmer–Lemeshow statistics),  precision and recall graph should be observed,   Matthews correlation coefficient(MCC) 
chart of all thresholds should be evaluated. 
MCC is a better measure of performance than F1 score especially in the imbalanced populations 

For imbalanced datasets, the best metric is the average precision, which essentially 
translates to the precision-recall curve. Unfortunately, this is not used by the majority.
In saying this, if one compares previous performances, then, one is not able to because they haven't 
used the metric. Unless, one goes back and actually performs evaluation using this metric, then comes back and 
compares, accordingly. This is unfortunately not possible most times. 

https://www.data-to-viz.com

